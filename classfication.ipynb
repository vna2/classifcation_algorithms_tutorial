{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition for Imbalanced Data\n",
    "\n",
    "A challenge for beginners working with imbalanced classification problems is what a specific skewed class distribution means. For example, what is the difference and implication for a 1:10 vs. a 1:100 class ratio?\n",
    "\n",
    "The make_classification() scikit-learn function can be used to define a synthetic dataset with a desired class imbalance. The “weights” argument specifies the ratio of examples in the negative class, e.g. [0.99, 0.01] means that 99 percent of the examples will belong to the majority class, and the remaining 1 percent will belong to the minority class.\n",
    "\n",
    "Once defined, we can summarize the class distribution using a Counter object to get an idea of exactly how many examples belong to each class.\n",
    "\n",
    "We can also create a scatter plot of the dataset because there are only two input variables. The dots can then be colored by each class. This plot provides a visual intuition for what exactly a 99 percent vs. 1 percent majority/minority class imbalance looks like in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 850, 1: 150})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot imbalanced classification problem\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.80, 0.10], flip_y=0)\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "    row_ix = where(y == label)[0]\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Imbalanced Classification Models\n",
    "Prediction accuracy is the most common metric for classification tasks, although it is inappropriate and potentially dangerously misleading when used on imbalanced classification tasks.\n",
    "\n",
    "The reason for this is because if 98 percent of the data belongs to the negative class, you can achieve 98 percent accuracy on average by simply predicting the negative class all the time, achieving a score that naively looks good, but in practice has no skill.\n",
    "\n",
    "Instead, alternate performance metrics must be adopted.\n",
    "\n",
    "Popular alternatives are the precision and recall scores that allow the performance of the model to be considered by focusing on the minority class, called the positive class.\n",
    "\n",
    "Precision calculates the ratio of the number of correctly predicted positive examples divided by the total number of positive examples that were predicted. Maximizing the precision will minimize the false positives.\n",
    "\n",
    "    Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "\n",
    "Recall predicts the ratio of the total number of correctly predicted positive examples divided by the total number of positive examples that could have been predicted. Maximizing recall will minimize false negatives.\n",
    "\n",
    "    Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "\n",
    "The performance of a model can be summarized by a single score that averages both the precision and the recall, called the F-Measure. Maximizing the F-Measure will maximize both the precision and recall at the same time.\n",
    "\n",
    "    F-measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "The example below fits a logistic regression model on an imbalanced classification problem and calculates the accuracy, which can then be compared to the precision, recall, and F-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n",
      "Precision: 1.000\n",
      "Recall: 0.960\n",
      "F-measure: 0.980\n"
     ]
    }
   ],
   "source": [
    "# evaluate imbalanced classification model with different metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0)\n",
    "# split into train/test sets with same class ratio\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "# define model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "# fit model\n",
    "model.fit(trainX, trainy)\n",
    "# predict on test set\n",
    "yhat = model.predict(testX)\n",
    "# evaluate predictions\n",
    "print('Accuracy: %.3f' % accuracy_score(testy, yhat))\n",
    "print('Precision: %.3f' % precision_score(testy, yhat))\n",
    "print('Recall: %.3f' % recall_score(testy, yhat))\n",
    "print('F-measure: %.3f' % f1_score(testy, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling the Majority Class \n",
    "\n",
    "A simple approach to using standard machine learning algorithms on an imbalanced dataset is to change the training dataset to have a more balanced class distribution.\n",
    "\n",
    "This can be achieved by deleting examples from the majority class, referred to as “undersampling.” A possible downside is that examples from the majority class that are helpful during modeling may be deleted.\n",
    "\n",
    "A fast and reliable approach is to randomly delete examples from the majority class to reduce the imbalance to a ratio that is less severe or even so that the classes are even.\n",
    "\n",
    "The example below creates a synthetic imbalanced classification data, then uses RandomUnderSampler class to change the class distribution from 1:100 minority to majority classes to the less severe 1:2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 9900, 1: 100})\n",
      "Counter({0: 200, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "# example of undersampling the majority class\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99, 0.01], flip_y=0)\n",
    "# summarize class distribution\n",
    "print(Counter(y))\n",
    "# define undersample strategy\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "# summarize class distribution\n",
    "print(Counter(y_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling the Minority Class\n",
    "\n",
    "An alternative to deleting examples from the majority class is to add new examples from the minority class.\n",
    "\n",
    "This can be achieved by simply duplicating examples in the minority class, but these examples do not add any new information. Instead, new examples from the minority can be synthesized using existing examples in the training dataset. These new examples will be “close” to existing examples in the feature space, but different in small but random ways.\n",
    "\n",
    "The SMOTE algorithm is a popular approach for oversampling the minority class. This technique can be used to reduce the imbalance or to make the class distribution even.\n",
    "\n",
    "The example below demonstrates using the SMOTE class provided by the imbalanced-learn library on a synthetic dataset. The initial class distribution is 1:100 and the minority class is oversampled to a 1:2 distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 9900, 1: 100})\n",
      "Counter({0: 9900, 1: 4950})\n"
     ]
    }
   ],
   "source": [
    "# example of oversampling the minority class\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99, 0.01], flip_y=0)\n",
    "# summarize class distribution\n",
    "print(Counter(y))\n",
    "# define oversample strategy\n",
    "oversample = SMOTE(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "# summarize class distribution\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data Undersampling and Oversampling\n",
    "\n",
    "Data undersampling will delete examples from the majority class, whereas data oversampling will add examples to the minority class. These two approaches can be combined and used on a single training dataset.\n",
    "\n",
    "Given that there are so many different data sampling techniques to choose from, it can be confusing as to which methods to combine. Thankfully, there are common combinations that have been shown to work well in practice; some examples include:\n",
    "\n",
    "    Random Undersampling with SMOTE oversampling.\n",
    "    Tomek Links Undersampling with SMOTE oversampling.\n",
    "    Edited Nearest Neighbors Undersampling with SMOTE oversampling.\n",
    "\n",
    "These combinations can be applied manually to a given training dataset by first applying one sampling algorithm, then another. Thankfully, the imbalanced-learn library provides implementations of common combined data sampling techniques.\n",
    "\n",
    "The example below demonstrates how to use the SMOTEENN that combines both SMOTE oversampling of the minority class and Edited Nearest Neighbors undersampling of the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 9900, 1: 100})\n",
      "Counter({0: 9692, 1: 4757})\n"
     ]
    }
   ],
   "source": [
    "# example of both undersampling and oversampling\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.combine import SMOTEENN\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99, 0.01], flip_y=0)\n",
    "# summarize class distribution\n",
    "print(Counter(y))\n",
    "# define sampling strategy\n",
    "sample = SMOTEENN(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = sample.fit_resample(X, y)\n",
    "# summarize class distribution\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost-Sensitive Algorithms\n",
    "\n",
    "Most machine learning algorithms assume that all misclassification errors made by a model are equal. This is often not the case for imbalanced classification problems, where missing a positive or minority class case is worse than incorrectly classifying an example from the negative or majority class.\n",
    "\n",
    "Cost-sensitive learning is a subfield of machine learning that takes the costs of prediction errors (and potentially other costs) into account when training a machine learning model. Many machine learning algorithms can be updated to be cost-sensitive, where the model is penalized for misclassification errors from one class more than the other, such as the minority class.\n",
    "\n",
    "The scikit-learn library provides this capability for a range of algorithms via the class_weight attribute specified when defining the model. A weighting can be specified that is inversely proportional to the class distribution.\n",
    "\n",
    "If the class distribution was 0.99 to 0.01 for the majority and minority classes, then the class_weight argument could be defined as a dictionary that defines a penalty of 0.01 for errors made for the majority class and a penalty of 0.99 for errors made with the minority class, e.g. {0:0.01, 1:0.99}.\n",
    "\n",
    "This is a useful heuristic and can be configured automatically by setting the class_weight argument to the string ‘balanced‘.\n",
    "\n",
    "The example below demonstrates how to define and fit a cost-sensitive logistic regression model on an imbalanced classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure: 0.571\n"
     ]
    }
   ],
   "source": [
    "# example of cost sensitive logistic regression for imbalanced classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0)\n",
    "# split into train/test sets with same class ratio\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "# define model\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "# fit model\n",
    "model.fit(trainX, trainy)\n",
    "# predict on test set\n",
    "yhat = model.predict(testX)\n",
    "# evaluate predictions\n",
    "print('F-Measure: %.3f' % f1_score(testy, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
